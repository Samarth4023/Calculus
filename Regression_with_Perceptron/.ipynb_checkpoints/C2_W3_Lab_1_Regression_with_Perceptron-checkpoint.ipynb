{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAt-K2qgcIou"
   },
   "source": [
    "# Regression with Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZYK-0rin5x7"
   },
   "source": [
    "In the week 2 assignment, you implemented the gradient descent method to build a linear regression model, predicting sales given a TV marketing budget. In this lab, you will construct a neural network corresponding to the same simple linear regression model. Then you will train the network, implementing the gradient descent method. After that you will increase the complexity of the neural network to build a multiple linear regression model, predicting house prices based on their size and quality.\n",
    "\n",
    "*Note*: The same models were discussed in Course 1 \"Linear Algebra\" week 3 assignment, but model training with backward propagation was omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Simple Linear Regression](#1)\n",
    "  - [ 1.1 - Simple Linear Regression Model](#1.1)\n",
    "  - [ 1.2 - Neural Network Model with a Single Perceptron and One Input Node](#1.2)\n",
    "  - [ 1.3 - Dataset](#1.3)\n",
    "- [ 2 - Implementation of the Neural Network Model for Linear Regression](#2)\n",
    "  - [ 2.1 - Defining the Neural Network Structure](#2.1)\n",
    "  - [ 2.2 - Initialize the Model's Parameters](#2.2)\n",
    "  - [ 2.3 - The Loop](#2.3)\n",
    "  - [ 2.4 - Integrate parts 2.1, 2.2 and 2.3 in nn_model() and make predictions](#2.4)\n",
    "- [ 3 - Multiple Linear Regression](#3)\n",
    "  - [ 3.1 - Multipe Linear Regression Model](#3.1)\n",
    "  - [ 3.2 - Neural Network Model with a Single Perceptron and Two Input Nodes](#3.2)\n",
    "  - [ 3.3 - Dataset](#3.3)\n",
    "  - [ 3.4 - Performance of the Neural Network Model for Multiple Linear Regression](#3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI8PBrk_2Z4V"
   },
   "source": [
    "## Packages\n",
    "\n",
    "Let's first import all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# A library for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "# Output of plotting commands is displayed inline within the Jupyter notebook.\n",
    "%matplotlib inline \n",
    "\n",
    "# Set a seed so that the results are consistent.\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize data\n",
    "def normalize(data):\n",
    "    if isinstance(data, pd.Series) or isinstance(data, pd.DataFrame):\n",
    "        data = data.values  # Convert to NumPy array if Pandas object\n",
    "    return (data - np.mean(data)) / np.std(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 - Simple Linear Regression Model\n",
    "\n",
    "You can describe a simple linear regression model as\n",
    "\n",
    "$$\\hat{y} = wx + b,\\tag{1}$$\n",
    "\n",
    "where $\\hat{y}$ is a prediction of dependent variable $y$ based on independent variable $x$ using a line equation with the slope $w$ and intercept $b$. \n",
    "\n",
    "Given a set of training data points $(x_1, y_1)$, ..., $(x_m, y_m)$, you will find the \"best\" fitting line - such parameters $w$ and $b$ that the differences between original values $y_i$ and predicted values $\\hat{y}_i = wx_i + b$ are minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 - Neural Network Model with a Single Perceptron and One Input Node\n",
    "\n",
    "The simplest neural network model that describes the above problem can be realized by using one **perceptron**. The **input** and **output** layers will have one **node** each ($x$ for input and $\\hat{y} = z$ for output):\n",
    "\n",
    "<img src=\"images/nn_model_linear_regression_simple.png\" style=\"width:400px;\">\n",
    "\n",
    "**Weight** ($w$) and **bias** ($b$) are the parameters that will get updated when you **train** the model. They are initialized to some random values or set to 0 and updated as the training progresses.\n",
    "\n",
    "For each training example $x^{(i)}$, the prediction $\\hat{y}^{(i)}$ can be calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "z^{(i)} &=  w x^{(i)} + b,\\\\\n",
    "\\hat{y}^{(i)} &= z^{(i)},\n",
    "\\tag{2}\\end{align}\n",
    "\n",
    "where $i = 1, \\dots, m$.\n",
    "\n",
    "You can organise all training examples as a vector $X$ of size ($1 \\times m$) and perform scalar multiplication of $X$ ($1 \\times m$) by a scalar $w$, adding $b$, which will be broadcasted to a vector of size ($1 \\times m$):\n",
    "\n",
    "\\begin{align}\n",
    "Z &=  w X + b,\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\tag{3}\\end{align}\n",
    "\n",
    "This set of calculations is called **forward propagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each training example you can measure the difference between original values $y^{(i)}$ and predicted values $\\hat{y}^{(i)}$ with the **loss function** $L\\left(w, b\\right)  = \\frac{1}{2}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$. Division by $2$ is taken just for scaling purposes, you will see the reason below, calculating partial derivatives. To compare the resulting vector of the predictions $\\hat{Y}$ ($1 \\times m$) with the vector $Y$ of original values $y^{(i)}$, you can take an average of the loss function values for each of the training examples:\n",
    "\n",
    "$$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.\\tag{4}$$\n",
    "\n",
    "This function is called the sum of squares **cost function**. The aim is to optimize the cost function during the training, which will minimize the differences between original values $y^{(i)}$ and predicted values $\\hat{y}^{(i)}$.\n",
    "\n",
    "When your weights were just initialized with some random values, and no training was done yet, you can't expect good results. You need to calculate the adjustments for the weight and bias, minimizing the cost function. This process is called **backward propagation**. \n",
    "\n",
    "According to the gradient descent algorithm, you can calculate partial derivatives as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w } &= \n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right).\n",
    "\\tag{5}\\end{align}\n",
    "\n",
    "You can see how the additional division by $2$ in the equation $(4)$ helped to simplify the results of the partial derivatives. Then update the parameters iteratively using the expressions\n",
    "\n",
    "\\begin{align}\n",
    "w &= w - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial w },\\\\\n",
    "b &= b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b },\n",
    "\\tag{6}\\end{align}\n",
    "\n",
    "where $\\alpha$ is the learning rate. Then repeat the process until the cost function stops decreasing.\n",
    "\n",
    "The general **methodology** to build a neural network is to:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation (calculate the perceptron output),\n",
    "    - Implement backward propagation (to get the required corrections for the parameters),\n",
    "    - Update parameters.\n",
    "4. Make predictions.\n",
    "\n",
    "You often build helper functions to compute steps 1-3 and then merge them into one function `nn_model()`. Once you've built `nn_model()` and learnt the right parameters, you can make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 - Dataset\n",
    "\n",
    "Load the [Kaggle dataset](https://www.kaggle.com/code/devzohaib/simple-linear-regression/notebook), saved in a file `data/tvmarketing.csv`. It has two fields: TV marketing expenses (`TV`) and sales amount (`Sales`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "path = \"data/tvmarketing.csv\"\n",
    "\n",
    "adv = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Sales\n",
       "0  230.1   22.1\n",
       "1   44.5   10.4\n",
       "2   17.2    9.3\n",
       "3  151.5   18.5\n",
       "4  180.8   12.9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adv.plot(x='TV', y='Sales', kind='scatter', c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields `TV` and `Sales` have different units. Remember that in the week 2 assignment to make gradient descent algorithm efficient, you needed to normalize each of them: subtract the mean value of the array from each of the elements in the array and divide them by the standard deviation.\n",
    "\n",
    "Column-wise normalization of the dataset can be done for all of the fields at once and is implemented in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\example\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3643: FutureWarning: The behavior of DataFrame.std with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "adv_norm = (adv - np.mean(adv))/np.std(adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data, you can see that it looks similar after normalization, but the values on the axes have changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adv_norm.plot(x='TV', y='Sales', kind='scatter', c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the fields into variables `X_norm` and `Y_norm` and reshape them to row vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_norm: (1, 200)\n",
      "The shape of Y_norm: (1, 200)\n",
      "I have m = 200 training examples!\n"
     ]
    }
   ],
   "source": [
    "X_norm = adv_norm['TV'].values\n",
    "Y_norm = adv_norm['Sales'].values\n",
    "\n",
    "#X_norm = np.array(X_norm).reshape((1, len(X_norm)))\n",
    "#Y_norm = np.array(Y_norm).reshape((1, len(Y_norm)))\n",
    "\n",
    "# Normalizing the data\n",
    "X_norm = normalize(X_norm).reshape(1, -1)\n",
    "Y_norm = normalize(Y_norm).reshape(1, -1)\n",
    "\n",
    "print ('The shape of X_norm: ' + str(X_norm.shape))\n",
    "print ('The shape of Y_norm: ' + str(Y_norm.shape))\n",
    "print ('I have m = %d training examples!' % (X_norm.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Implementation of the Neural Network Model for Linear Regression\n",
    "\n",
    "Setup the neural network in a way which will allow to extend this simple case of a model with a single perceptron and one input node to more complicated structures later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Defining the Neural Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two variables:\n",
    "- `n_x`: the size of the input layer\n",
    "- `n_y`: the size of the output layer\n",
    "\n",
    "using shapes of arrays `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 1\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    return (n_x, n_y)\n",
    "\n",
    "(n_x, n_y) = layer_sizes(X_norm, Y_norm)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Initialize the Model's Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `initialize_parameters()`, initializing the weights array of shape $(n_y \\times n_x) = (1 \\times 1)$ with random values and the bias vector of shape $(n_y \\times 1) = (1 \\times 1)$ with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.01788628]]\n",
      "b = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W -- weight matrix of shape (n_y, n_x)\n",
    "                    b -- bias value set as a vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    W = np.random.randn(n_y, n_x) * 0.01\n",
    "    b = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_y)\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - The Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `forward_propagation()` following the equation $(3)$ in the section [1.2](#1.2):\n",
    "\\begin{align}\n",
    "Z &=  w X + b\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some elements of output vector Y_hat: [ 0.01734705 -0.02141661 -0.02711838  0.00093098  0.00705046]\n"
     ]
    }
   ],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- The output\n",
    "    \"\"\"\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Forward Propagation to calculate Z.\n",
    "    Z = np.matmul(W, X) + b\n",
    "    Y_hat = Z\n",
    "\n",
    "    return Y_hat\n",
    "\n",
    "Y_hat = forward_propagation(X_norm, parameters)\n",
    "\n",
    "print(\"Some elements of output vector Y_hat:\", Y_hat[0, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your weights were just initialized with some random values, so the model has not been trained yet. \n",
    "\n",
    "Define a cost function $(4)$ which will be used to train the model:\n",
    "\n",
    "$$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.48616887080159715\n"
     ]
    }
   ],
   "source": [
    "def compute_cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function as a sum of squares\n",
    "    \n",
    "    Arguments:\n",
    "    Y_hat -- The output of the neural network of shape (n_y, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- sum of squares scaled by 1/(2*number of examples)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Number of examples.\n",
    "    m = Y_hat.shape[1]\n",
    "\n",
    "    # Compute the cost function.\n",
    "    cost = np.sum((Y_hat - Y)**2)/(2*m)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(Y_hat, Y_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "source": [
    "Calculate partial derivatives as shown in $(5)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w } &= \n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[-0.76433814]]\n",
      "db = [[-5.01820807e-16]]\n"
     ]
    }
   ],
   "source": [
    "def backward_propagation(Y_hat, X, Y):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation, calculating gradients\n",
    "    \n",
    "    Arguments:\n",
    "    Y_hat -- the output of the neural network of shape (n_y, number of examples)\n",
    "    X -- input data of shape (n_x, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Backward propagation: calculate partial derivatives denoted as dW, db for simplicity. \n",
    "    dZ = Y_hat - Y\n",
    "    dW = 1/m * np.dot(dZ, X.T)\n",
    "    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "grads = backward_propagation(Y_hat, X_norm, Y_norm)\n",
    "\n",
    "print(\"dW = \" + str(grads[\"dW\"]))\n",
    "print(\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters as shown in $(6)$:\n",
    "\n",
    "\\begin{align}\n",
    "w &= w - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial w },\\\\\n",
    "b &= b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b }.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W updated = [[0.93509205]]\n",
      "b updated = [[6.02184969e-16]]\n"
     ]
    }
   ],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    grads -- python dictionary containing gradients \n",
    "    learning_rate -- learning rate parameter for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\".\n",
    "    dW = grads[\"dW\"]\n",
    "    db = grads[\"db\"]\n",
    "    \n",
    "    # Update rule for each parameter.\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters_updated = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W updated = \" + str(parameters_updated[\"W\"]))\n",
    "print(\"b updated = \" + str(parameters_updated[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 - Integrate parts 2.1, 2.2 and 2.3 in nn_model() and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your neural network model in `nn_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, num_iterations=100, learning_rate=0.01, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a simple neural network model with gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data of shape (n_x, m)\n",
    "    Y -- true labels vector of shape (1, m)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- if True, print the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model, which can be used for prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0]  # size of input layer\n",
    "    n_y = Y.shape[0]  # size of output layer\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_y)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        Y_hat = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(Y_hat, Y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(Y_hat, X, Y)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.502173645482497\n",
      "W = [[0.78222442]]\n",
      "b = [[5.38236122e-16]]\n"
     ]
    }
   ],
   "source": [
    "parameters_simple = nn_model(X_norm, Y_norm, num_iterations=30, learning_rate=1.2, print_cost=True)\n",
    "print(\"W = \" + str(parameters_simple[\"W\"]))\n",
    "print(\"b = \" + str(parameters_simple[\"b\"]))\n",
    "\n",
    "W_simple = parameters[\"W\"]\n",
    "b_simple = parameters[\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that after a few iterations the cost function does not change anymore (the model converges).\n",
    "\n",
    "*Note*: This is a very simple model. In reality the models do not converge that quickly.\n",
    "\n",
    "The final model parameters can be used for making predictions, but don't forget about normalization and denormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV marketing expenses:\n",
      "[ 50 120 280]\n",
      "Predictions of sales:\n",
      "[ 9.40942557 12.7369904  20.34285287]\n"
     ]
    }
   ],
   "source": [
    "def predict(X, Y, parameters, X_pred):\n",
    "    \"\"\"\n",
    "    Predicts the output given new input data X_pred using parameters of the trained model.\n",
    "\n",
    "    Arguments:\n",
    "    X -- original input data used for normalization (Pandas Series or NumPy array)\n",
    "    Y -- original target data used for normalization (Pandas Series or NumPy array)\n",
    "    parameters -- parameters learnt by the model\n",
    "    X_pred -- input data for prediction (NumPy array)\n",
    "\n",
    "    Returns:\n",
    "    Y_pred -- predicted output (NumPy array)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from the dictionary \"parameters\"\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Convert X and Y to NumPy arrays if they are Pandas Series\n",
    "    if isinstance(X, pd.Series):\n",
    "        X = X.values\n",
    "    if isinstance(Y, pd.Series):\n",
    "        Y = Y.values\n",
    "    \n",
    "    # Normalize X_pred using the same mean and std as X\n",
    "    X_mean = np.mean(X)\n",
    "    X_std = np.std(X)\n",
    "    X_pred_norm = (X_pred - X_mean) / X_std\n",
    "    \n",
    "    # Ensure W is a proper shape for matrix multiplication\n",
    "    if W.shape[0] != X_pred_norm.shape[0]:\n",
    "        raise ValueError(f\"Number of features in W ({W.shape[0]}) does not match X_pred ({X_pred_norm.shape[0]})\")\n",
    "    \n",
    "    # Make predictions\n",
    "    Y_pred_norm = np.dot(W.T, X_pred_norm) + b\n",
    "    \n",
    "    # Denormalize Y_pred_norm to get Y_pred\n",
    "    Y_pred = Y_pred_norm * np.std(Y) + np.mean(Y)\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "#X_pred = np.array([50, 120, 280])\n",
    "#Y_pred = predict(adv[\"TV\"], adv[\"Sales\"], parameters_simple, X_pred)\n",
    "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
    "print(f\"Predictions of sales:\\n{Y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the linear regression line and some predictions. The regression line is red and the predicted points are blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features in W (1) does not match X_pred (3254)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m X_data \u001b[38;5;241m=\u001b[39m adv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTV\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(adv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTV\u001b[39m\u001b[38;5;124m\"\u001b[39m], pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;28;01melse\u001b[39;00m adv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTV\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m X_line \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(np\u001b[38;5;241m.\u001b[39mmin(X_data), np\u001b[38;5;241m.\u001b[39mmax(X_data) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m Y_line \u001b[38;5;241m=\u001b[39m predict(X_data, adv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m\"\u001b[39m], parameters_simple, X_line)\n\u001b[0;32m     12\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(X_line, Y_line, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming X_pred and Y_pred are defined earlier\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 32\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(X, Y, parameters, X_pred)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Ensure W is a proper shape for matrix multiplication\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m X_pred_norm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features in W (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match X_pred (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_pred_norm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     35\u001b[0m Y_pred_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(W\u001b[38;5;241m.\u001b[39mT, X_pred_norm) \u001b[38;5;241m+\u001b[39m b\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features in W (1) does not match X_pred (3254)"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(adv[\"TV\"], adv[\"Sales\"], color=\"black\")\n",
    "\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "\n",
    "# Convert adv[\"TV\"] to a NumPy array if it's a Pandas Series\n",
    "X_data = adv[\"TV\"].values if isinstance(adv[\"TV\"], pd.Series) else adv[\"TV\"]\n",
    "\n",
    "X_line = np.arange(np.min(X_data), np.max(X_data) * 1.1, 0.1)\n",
    "Y_line = predict(X_data, adv[\"Sales\"], parameters_simple, X_line)\n",
    "ax.plot(X_line, Y_line, \"r\")\n",
    "\n",
    "# Assuming X_pred and Y_pred are defined earlier\n",
    "ax.plot(X_pred, Y_pred, \"bo\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's increase the number of the input nodes to build a multiple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Multipe Linear Regression Model\n",
    "\n",
    "You can write a multiple linear regression model with two independent variables $x_1$, $x_2$ as\n",
    "\n",
    "$$\\hat{y} = w_1x_1 + w_2x_2 + b = Wx + b,\\tag{7}$$\n",
    "\n",
    "where $Wx$ is the dot product of the input vector $x = \\begin{bmatrix} x_1 & x_2\\end{bmatrix}$ and the parameters vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$, scalar parameter $b$ is the intercept. The goal of the training process is to find the \"best\" parameters $w_1$, $w_2$ and $b$ such that the differences between original values $y_i$ and predicted values $\\hat{y}_i$ are minimum for the given training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Neural Network Model with a Single Perceptron and Two Input Nodes\n",
    "\n",
    "To describe the multiple regression problem, you can still use a model with one perceptron, but this time you need two input nodes, as shown in the following scheme:\n",
    "\n",
    "<img src=\"images/nn_model_linear_regression_multiple.png\" style=\"width:420px;\">\n",
    "\n",
    "The perceptron output calculation for every training example $x^{(i)} = \\begin{bmatrix} x_1^{(i)} & x_2^{(i)}\\end{bmatrix}$ can be written with dot product:\n",
    "\n",
    "$$z^{(i)} = w_1x_1^{(i)} + w_2x_2^{(i)} + b = Wx^{(i)} + b,\\tag{8}$$\n",
    "\n",
    "where weights are in the vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$ and bias $b$ is a scalar. The output layer will have the same single node $\\hat{y} = z$.\n",
    "\n",
    "Organise all training examples in a matrix $X$ of a shape ($2 \\times m$), putting $x_1^{(i)}$ and $x_2^{(i)}$ into columns. Then matrix multiplication of $W$ ($1 \\times 2$) and $X$ ($2 \\times m$) will give a ($1 \\times m$) vector\n",
    "\n",
    "$$WX = \n",
    "\\begin{bmatrix} w_1 & w_2\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "x_1^{(1)} & x_1^{(2)} & \\dots & x_1^{(m)} \\\\ \n",
    "x_2^{(1)} & x_2^{(2)} & \\dots & x_2^{(m)} \\\\ \\end{bmatrix}\n",
    "=\\begin{bmatrix} \n",
    "w_1x_1^{(1)} + w_2x_2^{(1)} & \n",
    "w_1x_1^{(2)} + w_2x_2^{(2)} & \\dots & \n",
    "w_1x_1^{(m)} + w_2x_2^{(m)}\\end{bmatrix}.$$\n",
    "\n",
    "And the model can be written as\n",
    "\n",
    "\\begin{align}\n",
    "Z &=  W X + b,\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\tag{9}\\end{align}\n",
    "\n",
    "where $b$ is broadcasted to the vector of size ($1 \\times m$). These are the calculations to perform in the forward propagation step. Cost function will remain the same (see equation $(4)$ in the section [1.2](#1.2)):\n",
    "\n",
    "$$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the gradient descent algorithm, you can calculate cost function partial derivatives as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_1 } &= \n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x_1^{(i)},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_2 } &= \n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x_2^{(i)},\\tag{10}\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \n",
    "\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right).\n",
    "\\end{align}\n",
    "\n",
    "After performing the forward propagation as shown in $(9)$, the variable $\\hat{Y}$ will contain the predictions in the array of size ($1 \\times m$). The original values $y^{(i)}$ will be kept in the array $Y$ of the same size. Thus, $\\left(\\hat{Y} - Y\\right)$ will be a ($1 \\times m$) array containing differences $\\left(\\hat{y}^{(i)} - y^{(i)}\\right)$. Matrix $X$ of size ($2 \\times m$) has all $x_1^{(i)}$ values in the first row and $x_2^{(i)}$ in the second row. Thus, the sums in the first two equations of $(10)$ can be calculated as matrix multiplication of $\\left(\\hat{Y} - Y\\right)$ of a shape ($1 \\times m$) and $X^T$ of a shape ($m \\times 2$), resulting in the ($1 \\times 2$) array:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial W } = \n",
    "\\begin{bmatrix} \\frac{\\partial \\mathcal{L} }{ \\partial w_1 } & \n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_2 }\\end{bmatrix} = \\frac{1}{m}\\left(\\hat{Y} - Y\\right)X^T.\\tag{11}$$\n",
    "\n",
    "Similarly for $\\frac{\\partial \\mathcal{L} }{ \\partial b }$:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial b } = \\frac{1}{m}\\left(\\hat{Y} - Y\\right)\\mathbf{1}.\\tag{12}$$\n",
    "\n",
    "where $\\mathbf{1}$ is just a ($m \\times 1$) vector of ones.\n",
    "\n",
    "\n",
    "See how linear algebra and calculus work together to make calculations so nice and tidy! You can now update the parameters using matrix form of $W$:\n",
    "\n",
    "\\begin{align}\n",
    "W &= W - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W },\\\\\n",
    "b &= b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b },\n",
    "\\tag{13}\\end{align}\n",
    "\n",
    "where $\\alpha$ is a learning rate. Repeat the process in a loop until the cost function stops decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Dataset\n",
    "\n",
    "Let's build a linear regression model for a Kaggle dataset [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques), saved in a file `data/house_prices_train.csv`. You will use two fields - ground living area (`GrLivArea`, square feet) and rates of the overall quality of material and finish (`OverallQual`, 1-10) to predict sales price (`SalePrice`, dollars).\n",
    "\n",
    "To open the dataset you can use `pandas` function `read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/house_prices_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the required fields and save them in the variables `X_multi`, `Y_multi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multi = df[['GrLivArea', 'OverallQual']]\n",
    "Y_multi = df['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>OverallQual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1717</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2198</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1647</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2073</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2340</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1078</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1256</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GrLivArea  OverallQual\n",
       "0          1710            7\n",
       "1          1262            6\n",
       "2          1786            7\n",
       "3          1717            7\n",
       "4          2198            8\n",
       "...         ...          ...\n",
       "1455       1647            6\n",
       "1456       2073            6\n",
       "1457       2340            7\n",
       "1458       1078            5\n",
       "1459       1256            5\n",
       "\n",
       "[1460 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_multi)\n",
    "display(Y_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\example\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3643: FutureWarning: The behavior of DataFrame.std with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_multi_norm = (X_multi - np.mean(X_multi))/np.std(X_multi)\n",
    "Y_multi_norm = (Y_multi - np.mean(Y_multi))/np.std(Y_multi)\n",
    "Y_multi_array = Y_multi.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert results to the `NumPy` arrays, transpose `X_multi_norm` to get an array of a shape ($2 \\times m$) and reshape `Y_multi_norm` to bring it to the shape ($1 \\times m$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X: (2, 1460)\n",
      "The shape of Y: (1, 1460)\n",
      "I have m = 1460 training examples!\n"
     ]
    }
   ],
   "source": [
    "X_multi_norm = normalize(X_multi).T\n",
    "Y_multi_norm = normalize(Y_multi_array).reshape(1, -1)\n",
    "\n",
    "print ('The shape of X: ' + str(X_multi_norm.shape))\n",
    "print ('The shape of Y: ' + str(Y_multi_norm.shape))\n",
    "print ('I have m = %d training examples!' % (X_multi_norm.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Performance of the Neural Network Model for Multiple Linear Regression\n",
    "\n",
    "Now... you do not need to change anything in your neural network implementation! Go through the code in section [2](#2) and see that if you pass new datasets `X_multi_norm` and `Y_multi_norm`, the input layer size $n_x$ will get equal to $2$ and the rest of the implementation will remain exactly the same, even the backward propagation!\n",
    "\n",
    "Train the model for $100$ iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.499752\n",
      "Cost after iteration 1: 0.497937\n",
      "Cost after iteration 2: 0.496161\n",
      "Cost after iteration 3: 0.494420\n",
      "Cost after iteration 4: 0.492713\n",
      "Cost after iteration 5: 0.491040\n",
      "Cost after iteration 6: 0.489398\n",
      "Cost after iteration 7: 0.487785\n",
      "Cost after iteration 8: 0.486202\n",
      "Cost after iteration 9: 0.484645\n",
      "Cost after iteration 10: 0.483115\n",
      "Cost after iteration 11: 0.481610\n",
      "Cost after iteration 12: 0.480130\n",
      "Cost after iteration 13: 0.478672\n",
      "Cost after iteration 14: 0.477236\n",
      "Cost after iteration 15: 0.475821\n",
      "Cost after iteration 16: 0.474427\n",
      "Cost after iteration 17: 0.473052\n",
      "Cost after iteration 18: 0.471696\n",
      "Cost after iteration 19: 0.470357\n",
      "Cost after iteration 20: 0.469037\n",
      "Cost after iteration 21: 0.467732\n",
      "Cost after iteration 22: 0.466444\n",
      "Cost after iteration 23: 0.465172\n",
      "Cost after iteration 24: 0.463914\n",
      "Cost after iteration 25: 0.462670\n",
      "Cost after iteration 26: 0.461441\n",
      "Cost after iteration 27: 0.460225\n",
      "Cost after iteration 28: 0.459021\n",
      "Cost after iteration 29: 0.457831\n",
      "Cost after iteration 30: 0.456652\n",
      "Cost after iteration 31: 0.455485\n",
      "Cost after iteration 32: 0.454330\n",
      "Cost after iteration 33: 0.453185\n",
      "Cost after iteration 34: 0.452051\n",
      "Cost after iteration 35: 0.450928\n",
      "Cost after iteration 36: 0.449814\n",
      "Cost after iteration 37: 0.448710\n",
      "Cost after iteration 38: 0.447616\n",
      "Cost after iteration 39: 0.446531\n",
      "Cost after iteration 40: 0.445455\n",
      "Cost after iteration 41: 0.444388\n",
      "Cost after iteration 42: 0.443329\n",
      "Cost after iteration 43: 0.442279\n",
      "Cost after iteration 44: 0.441237\n",
      "Cost after iteration 45: 0.440203\n",
      "Cost after iteration 46: 0.439176\n",
      "Cost after iteration 47: 0.438157\n",
      "Cost after iteration 48: 0.437146\n",
      "Cost after iteration 49: 0.436142\n",
      "Cost after iteration 50: 0.435145\n",
      "Cost after iteration 51: 0.434155\n",
      "Cost after iteration 52: 0.433171\n",
      "Cost after iteration 53: 0.432195\n",
      "Cost after iteration 54: 0.431225\n",
      "Cost after iteration 55: 0.430262\n",
      "Cost after iteration 56: 0.429305\n",
      "Cost after iteration 57: 0.428354\n",
      "Cost after iteration 58: 0.427409\n",
      "Cost after iteration 59: 0.426470\n",
      "Cost after iteration 60: 0.425538\n",
      "Cost after iteration 61: 0.424611\n",
      "Cost after iteration 62: 0.423690\n",
      "Cost after iteration 63: 0.422774\n",
      "Cost after iteration 64: 0.421865\n",
      "Cost after iteration 65: 0.420960\n",
      "Cost after iteration 66: 0.420062\n",
      "Cost after iteration 67: 0.419168\n",
      "Cost after iteration 68: 0.418280\n",
      "Cost after iteration 69: 0.417397\n",
      "Cost after iteration 70: 0.416519\n",
      "Cost after iteration 71: 0.415647\n",
      "Cost after iteration 72: 0.414779\n",
      "Cost after iteration 73: 0.413917\n",
      "Cost after iteration 74: 0.413059\n",
      "Cost after iteration 75: 0.412207\n",
      "Cost after iteration 76: 0.411359\n",
      "Cost after iteration 77: 0.410516\n",
      "Cost after iteration 78: 0.409677\n",
      "Cost after iteration 79: 0.408844\n",
      "Cost after iteration 80: 0.408015\n",
      "Cost after iteration 81: 0.407190\n",
      "Cost after iteration 82: 0.406371\n",
      "Cost after iteration 83: 0.405555\n",
      "Cost after iteration 84: 0.404744\n",
      "Cost after iteration 85: 0.403938\n",
      "Cost after iteration 86: 0.403136\n",
      "Cost after iteration 87: 0.402339\n",
      "Cost after iteration 88: 0.401545\n",
      "Cost after iteration 89: 0.400756\n",
      "Cost after iteration 90: 0.399972\n",
      "Cost after iteration 91: 0.399191\n",
      "Cost after iteration 92: 0.398415\n",
      "Cost after iteration 93: 0.397643\n",
      "Cost after iteration 94: 0.396875\n",
      "Cost after iteration 95: 0.396111\n",
      "Cost after iteration 96: 0.395351\n",
      "Cost after iteration 97: 0.394596\n",
      "Cost after iteration 98: 0.393844\n",
      "Cost after iteration 99: 0.393096\n",
      "W = [[0.29680903 0.06561422]]\n",
      "b = [[-0.09256262]]\n"
     ]
    }
   ],
   "source": [
    "parameters_multi = nn_model(X_multi_norm, Y_multi_norm, num_iterations=100, learning_rate=0.01, print_cost=True)\n",
    "\n",
    "print(\"W = \" + str(parameters_multi[\"W\"]))\n",
    "print(\"b = \" + str(parameters_multi[\"b\"]))\n",
    "\n",
    "W_multi = parameters_multi[\"W\"]\n",
    "b_multi = parameters_multi[\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (2,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X_pred_multi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1710\u001b[39m, \u001b[38;5;241m7\u001b[39m], [\u001b[38;5;241m1200\u001b[39m, \u001b[38;5;241m6\u001b[39m], [\u001b[38;5;241m2200\u001b[39m, \u001b[38;5;241m8\u001b[39m]])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m----> 2\u001b[0m Y_pred_multi \u001b[38;5;241m=\u001b[39m predict(X_multi, Y_multi, parameters_multi, X_pred_multi)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround living area, square feet:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mX_pred_multi[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRates of the overall quality of material and finish, 1-10:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mX_pred_multi[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(X, Y, parameters, X_pred)\u001b[0m\n\u001b[0;32m     11\u001b[0m     X_pred_norm \u001b[38;5;241m=\u001b[39m ((X_pred \u001b[38;5;241m-\u001b[39m X_mean)\u001b[38;5;241m/\u001b[39mX_std)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_pred)))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     X_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mmean(X))\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(X\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m]),\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     14\u001b[0m     X_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mstd(X))\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(X\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m]),\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     15\u001b[0m     X_pred_norm \u001b[38;5;241m=\u001b[39m ((X_pred \u001b[38;5;241m-\u001b[39m X_mean)\u001b[38;5;241m/\u001b[39mX_std)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (2,1)"
     ]
    }
   ],
   "source": [
    "X_pred_multi = np.array([[1710, 7], [1200, 6], [2200, 8]]).T\n",
    "Y_pred_multi = predict(X_multi, Y_multi, parameters_multi, X_pred_multi)\n",
    "\n",
    "print(f\"Ground living area, square feet:\\n{X_pred_multi[0]}\")\n",
    "print(f\"Rates of the overall quality of material and finish, 1-10:\\n{X_pred_multi[1]}\")\n",
    "print(f\"Predictions of sales price, $:\\n{np.round(Y_pred_multi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this lab!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W1_Assignment_Solution.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "grader_version": "1",
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
